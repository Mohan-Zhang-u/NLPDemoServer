{"data": [{"title": "", "paragraphs": [{"context": "[20] The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks. [22] The probabilistic interpretation was introduced by researchers including Hopfield, Widrow and Narendra and popularized in surveys such as the one by Bishop. [23]\n\nHistory\nThe term Deep Learning was introduced to the machine learning community by Rina Dechter in 1986,[24][13] and to artificial neural networks by Igor Aizenberg and colleagues in 2000, in the context of Boolean threshold neurons. [25][26]\n\nThe first general, working learning algorithm for supervised, deep, feedforward, multilayer perceptrons was published by Alexey Ivakhnenko and Lapa in 1965. [27] A 1971 paper described a deep network with 8 layers trained by the group method of data handling algorithm. [28]\n\nOther deep learning working architectures, specifically those built for computer vision, began with the Neocognitron introduced by Kunihiko Fukushima in 1980. [29] In 1989, Yann LeCun et al. applied the standard backpropagation algorithm, which had been around as the reverse mode of automatic differentiation since 1970,[30][31][32][33] to a deep neural network with the purpose of recognizing handwritten ZIP codes on mail. While the algorithm worked, training required 3 days. [34]\n\nBy 1991 such systems were used for recognizing isolated 2-D hand-written digits, while recognizing 3-D objects was done by matching 2-D images with a handcrafted 3-D object model. Weng et al. suggested that a human brain does not use a monolithic 3-D object model and in 1992 they published Cresceptron,[35][36][37] a method for performing 3-D object recognition in cluttered scenes. Because it directly used natural images, Cresceptron started the beginning of general-purpose visual learning for natural 3D worlds. Cresceptron is a cascade of layers similar to Neocognitron. But while Neocognitron required a human programmer to hand-merge features, Cresceptron learned an open number of features in each layer without supervision, where each feature is represented by a convolution kernel. Cresceptron segmented each learned object from a cluttered scene through back-analysis through the network. Max pooling, now often adopted by deep neural networks (e.g. ImageNet tests), was first used in Cresceptron to reduce the position resolution by a factor of (2x2) to 1 through the cascade for better generalization. In 1994, Andr\u00e9 de Carvalho, together with Mike Fairhurst and David Bisset, published experimental results of a multi-layer boolean neural network, also known as a weightless neural network, composed of a 3-layers self-organising feature extraction neural network module (SOFT) followed by a multi-layer classification neural network module (GSN), which were independently trained. Each layer in the feature extraction module extracted features with growing complexity regarding the previous layer. [38]\n\nIn 1995, Brendan Frey demonstrated that it was possible to train (over two days) a network containing six fully connected layers and several hundred hidden units using the wake-sleep algorithm, co-developed with Peter Dayan and Hinton. [39] Many factors contribute to the slow speed, including the vanishing gradient problem analyzed in 1991 by Sepp Hochreiter. \n", "qas": [{"answers": [{"answer_start": -1, "text": ""}], "question": "what is neural network?", "id": 0}]}]}, {"title": "", "paragraphs": [{"context": "Contents\n1\tHistory\n1.1\tHebbian learning\n1.2\tBackpropagation\n1.3\tHardware-based designs\n1.4\tContests\n1.5\tConvolutional networks\n2\tModels\n2.1\tComponents of an artificial neural network\n2.2\tNeural networks as functions\n2.3\tLearning\n2.4\tLearning paradigms\n2.5\tLearning algorithms\n3\tVariants\n3.1\tGroup method of data handling\n3.2\tConvolutional neural networks\n3.3\tLong short-term memory\n3.4\tDeep reservoir computing\n3.5\tDeep belief networks\n3.6\tLarge memory storage and retrieval neural networks\n3.7\tStacked (de-noising) auto-encoders\n3.8\tDeep stacking networks\n3.9\tTensor deep stacking networks\n3.10\tSpike-and-slab RBMs\n3.11\tCompound hierarchical-deep models\n3.12\tDeep predictive coding networks\n3.13\tNetworks with separate memory structures\n3.14\tMultilayer kernel machine\n4\tNeural architecture search\n5\tUse\n6\tApplications\n6.1\tTypes of models\n7\tTheoretical properties\n7.1\tComputational power\n7.2\tCapacity\n7.3\tConvergence\n7.4\tGeneralization and statistics\n8\tCriticism\n8.1\tTraining issues\n8.2\tTheoretical issues\n8.3\tHardware issues\n8.4\tPractical counterexamples to criticisms\n8.5\tHybrid approaches\n9\tTypes\n10\tGallery\n11\tSee also\n12\tReferences\n13\tBibliography\n14\tExternal links\nHistory\nWarren McCulloch and Walter Pitts[3] (1943) created a computational model for neural networks based on mathematics and algorithms called threshold logic. This model paved the way for neural network research to split into two approaches. One approach focused on biological processes in the brain while the other focused on the application of neural networks to artificial intelligence. This work led to work on nerve networks and their link to finite automata. [4]\n\nHebbian learning\nIn the late 1940s, D. O. Hebb[5] created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Hebbian learning is unsupervised learning. This evolved into models for long term potentiation. Researchers started applying these ideas to computational models in 1948 with Turing\\'s B-type machines. Farley and Clark[6] (1954) first used computational machines, then called \\\"calculators\\\", to simulate a Hebbian network. Other neural network computational machines were created by Rochester, Holland, Habit and Duda (1956). [7]\n\nRosenblatt[8] (1958) created the perceptron, an algorithm for pattern recognition. With mathematical notation, Rosenblatt described circuitry not in the basic perceptron, such as the exclusive-or circuit that could not be processed by neural networks at the time. [9]\n\nIn 1959, a biological model proposed by Nobel laureates Hubel and Wiesel was based on their discovery of two types of cells in the primary visual cortex: simple cells and complex cells. [10]\n\nThe first functional networks with many layers were published by Ivakhnenko and Lapa in 1965, becoming the Group Method of Data Handling. [11][12][13]\n\nNeural network research stagnated after machine learning research by Minsky and Papert (1969),[14] who discovered two key issues with the computational machines that processed neural networks. The first was that basic perceptrons were incapable of processing the exclusive-or circuit. The second was that computers didn\\'t have enough processing power to effectively handle the work required by large neural networks. Neural network research slowed until computers achieved far greater processing power. \n", "qas": [{"answers": [{"answer_start": -1, "text": ""}], "question": "what is neural network?", "id": 1}]}]}, {"title": "", "paragraphs": [{"context": "Neurons and synapses may also have a weight that varies as learning proceeds, which can increase or decrease the strength of the signal that it sends downstream. Typically, neurons are organized in layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first (input), to the last (output) layer, possibly after traversing the layers multiple times. The original goal of the neural network approach was to solve problems in the same way that a human brain would. Over time, attention focused on matching specific mental abilities, leading to deviations from biology such as backpropagation, or passing information in the reverse direction and adjusting the network to reflect that information. Neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis. As of 2017, neural networks typically have a few thousand to a few million units and millions of connections. Despite this number being several order of magnitude less than the number of neurons on a human brain, these networks can perform many tasks at a level beyond that of humans (e.g., recognizing faces, playing \\\"Go\\\"[99] ). Deep neural networks\n\nThis section may be too technical for most readers to understand. Please help improve it to make it understandable to non-experts, without removing the technical details. (July 2016) (Learn how and when to remove this template message)\nA deep neural network (DNN) is an artificial neural network (ANN) with multiple layers between the input and output layers. [11][2] The DNN finds the correct mathematical manipulation to turn the input into the output, whether it be a linear relationship or a non-linear relationship. The network moves through the layers calculating the probability of each output. For example, a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed. The user can review the results and select which probabilities the network should display (above a certain threshold, etc.) and return the proposed label. Each mathematical manipulation as such is considered a layer, and complex DNN have many layers, hence the name \\\"deep\\\" networks. The goal is that eventually, the network will be trained to decompose an image into features, identify trends that exist across all samples and classify new images by their similarities without requiring human input. [100]\n\nDNNs can model complex non-linear relationships. DNN architectures generate compositional models where the object is expressed as a layered composition of primitives. [101] The extra layers enable composition of features from lower layers, potentially modeling complex data with fewer units than a similarly performing shallow network. [11]\n\nDeep architectures include many variants of a few basic approaches. Each architecture has found success in specific domains. It is not always possible to compare the performance of multiple architectures, unless they have been evaluated on the same data sets. DNNs are typically feedforward networks in which data flows from the input layer to the output layer without looping back. \n", "qas": [{"answers": [{"answer_start": -1, "text": ""}], "question": "what is neural network?", "id": 2}]}]}], "version": "my_ver"}