{"id": "Question answering1549927015402.txt", "text": "In the 1970s, knowledge bases were developed that targeted narrower domains of knowledge. The QA systems developed to interface with these expert systems produced more repeatable and valid responses to questions within an area of knowledge. These expert systems closely resembled modern QA systems except in their internal architecture. Expert systems rely heavily on expert-constructed and organized knowledge bases, whereas many modern QA systems rely on statistical processing of a large, unstructured, natural language text corpus. The 1970s and 1980s saw the development of comprehensive theories in computational linguistics, which led to the development of ambitious projects in text comprehension and question answering. One example of such a system was the Unix Consultant (UC), developed by Robert Wilensky at U.C. Berkeley in the late 1980s. The system answered questions pertaining to the Unix operating system. It had a comprehensive hand-crafted knowledge base of its domain, and it aimed at phrasing the answer to accommodate various types of users. Another project was LILOG, a text-understanding system that operated on the domain of tourism information in a German city. The systems developed in the UC and LILOG projects never went past the stage of simple demonstrations, but they helped the development of theories on computational linguistics and reasoning. Recently, specialized natural language QA systems have been developed, such as EAGLi for health and life scientists. Architecture\r\nAs of 2001, QA systems typically included a question classifier module that determines the type of question and the type of answer. [4] A multiagent question-answering architecture has been proposed, where each domain is represented by an agent which tries to answer questions taking into account its specific knowledge; a meta\u2013agent controls the cooperation between question answering agents and chooses the most relevant answer(s). [5]\r\n\r\nQuestion answering methods\r\nQA is very dependent on a good search corpus - for without documents containing the answer, there is little any QA system can do. It thus makes sense that larger collection sizes generally lend well to better QA performance, unless the question domain is orthogonal to the collection. The notion of data redundancy in massive collections, such as the web, means that nuggets of information are likely to be phrased in many different ways in differing contexts and documents,[6] leading to two benefits:\r\n\r\nBy having the right information appear in many forms, the burden on the QA system to perform complex NLP techniques to understand the text is lessened. Correct answers can be filtered from false positives by relying on the correct answer to appear more times in the documents than instances of incorrect ones. Some question answering systems rely heavily on automated reasoning. [7][8] There are a number of question answering systems designed in Prolog,[9] a logic programming language associated with artificial intelligence. Open domain question answering\r\n\r\nThis section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (January 2016) (Learn how and when to remove this template message)\r\nIn information retrieval, an open domain question answering system aims at returning an answer in response to the user's question. The returned answer is in the form of short texts rather than a list of relevant documents. "}