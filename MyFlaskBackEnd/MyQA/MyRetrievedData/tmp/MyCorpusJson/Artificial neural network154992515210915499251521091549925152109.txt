{"id": "Artificial neural network154992515210915499251521091549925152109.txt", "text": "[31] Ciresan and colleagues (2010)[32] in Schmidhuber's group showed that despite the vanishing gradient problem, GPUs makes back-propagation feasible for many-layered feedforward neural networks. Contests\r\nBetween 2009 and 2012, recurrent neural networks and deep feedforward neural networks developed in Schmidhuber's research group won eight international competitions in pattern recognition and machine learning. [33][34] For example, the bi-directional and multi-dimensional long short-term memory (LSTM)[35][36][37][38] of Graves et al. won three competitions in connected handwriting recognition at the 2009 International Conference on Document Analysis and Recognition (ICDAR), without any prior knowledge about the three languages to be learned. [37][36]\r\n\r\nCiresan and colleagues won pattern recognition contests, including the IJCNN 2011 Traffic Sign Recognition Competition,[39] the ISBI 2012 Segmentation of Neuronal Structures in Electron Microscopy Stacks challenge[40] and others. Their neural networks were the first pattern recognizers to achieve human-competitive or even superhuman performance[41] on benchmarks such as traffic sign recognition (IJCNN 2012), or the MNIST handwritten digits problem. Researchers demonstrated (2010) that deep neural networks interfaced to a hidden Markov model with context-dependent states that define the neural network output layer can drastically reduce errors in large-vocabulary speech recognition tasks such as voice search. GPU-based implementations[42] of this approach won many pattern recognition contests, including the IJCNN 2011 Traffic Sign Recognition Competition,[39] the ISBI 2012 Segmentation of neuronal structures in EM stacks challenge,[40] the ImageNet Competition[43] and others. Deep, highly nonlinear neural architectures similar to the neocognitron[44] and the \"standard architecture of vision\",[45] inspired by simple and complex cells, were pre-trained by unsupervised methods by Hinton. [46][27] A team from his lab won a 2012 contest sponsored by Merck to design software to help find molecules that might identify new drugs. [47]\r\n\r\nConvolutional networks\r\nAs of 2011, the state of the art in deep learning feedforward networks alternated between convolutional layers and max-pooling layers,[42][48] topped by several fully or sparsely connected layers followed by a final classification layer. Learning is usually done without unsupervised pre-training. In the convolutional layer, there are filters that are convolved with the input. Each filter is equivalent to a weights vector that has to be trained. Such supervised deep learning methods were the first to achieve human-competitive performance on certain tasks. [41]\r\n\r\nArtificial neural networks were able to guarantee shift invariance to deal with small and large natural objects in large cluttered scenes, only when invariance extended beyond shift, to all ANN-learned concepts, such as location, type (object class label), scale, lighting and others. This was realized in Developmental Networks (DNs)[49] whose embodiments are Where-What Networks, WWN-1 (2008)[50] through WWN-7 (2013). [51]\r\n\r\nModels\r\n\r\nThis section may be confusing or unclear to readers. Please help us clarify the section. There might be a discussion about this on the talk page. "}