{"id": "Artificial neural network1549927015379.txt", "text": "Contents\r\n1\tHistory\r\n1.1\tHebbian learning\r\n1.2\tBackpropagation\r\n1.3\tHardware-based designs\r\n1.4\tContests\r\n1.5\tConvolutional networks\r\n2\tModels\r\n2.1\tComponents of an artificial neural network\r\n2.2\tNeural networks as functions\r\n2.3\tLearning\r\n2.4\tLearning paradigms\r\n2.5\tLearning algorithms\r\n3\tVariants\r\n3.1\tGroup method of data handling\r\n3.2\tConvolutional neural networks\r\n3.3\tLong short-term memory\r\n3.4\tDeep reservoir computing\r\n3.5\tDeep belief networks\r\n3.6\tLarge memory storage and retrieval neural networks\r\n3.7\tStacked (de-noising) auto-encoders\r\n3.8\tDeep stacking networks\r\n3.9\tTensor deep stacking networks\r\n3.10\tSpike-and-slab RBMs\r\n3.11\tCompound hierarchical-deep models\r\n3.12\tDeep predictive coding networks\r\n3.13\tNetworks with separate memory structures\r\n3.14\tMultilayer kernel machine\r\n4\tNeural architecture search\r\n5\tUse\r\n6\tApplications\r\n6.1\tTypes of models\r\n7\tTheoretical properties\r\n7.1\tComputational power\r\n7.2\tCapacity\r\n7.3\tConvergence\r\n7.4\tGeneralization and statistics\r\n8\tCriticism\r\n8.1\tTraining issues\r\n8.2\tTheoretical issues\r\n8.3\tHardware issues\r\n8.4\tPractical counterexamples to criticisms\r\n8.5\tHybrid approaches\r\n9\tTypes\r\n10\tGallery\r\n11\tSee also\r\n12\tReferences\r\n13\tBibliography\r\n14\tExternal links\r\nHistory\r\nWarren McCulloch and Walter Pitts[3] (1943) created a computational model for neural networks based on mathematics and algorithms called threshold logic. This model paved the way for neural network research to split into two approaches. One approach focused on biological processes in the brain while the other focused on the application of neural networks to artificial intelligence. This work led to work on nerve networks and their link to finite automata. [4]\r\n\r\nHebbian learning\r\nIn the late 1940s, D. O. Hebb[5] created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Hebbian learning is unsupervised learning. This evolved into models for long term potentiation. Researchers started applying these ideas to computational models in 1948 with Turing's B-type machines. Farley and Clark[6] (1954) first used computational machines, then called \"calculators\", to simulate a Hebbian network. Other neural network computational machines were created by Rochester, Holland, Habit and Duda (1956). [7]\r\n\r\nRosenblatt[8] (1958) created the perceptron, an algorithm for pattern recognition. With mathematical notation, Rosenblatt described circuitry not in the basic perceptron, such as the exclusive-or circuit that could not be processed by neural networks at the time. [9]\r\n\r\nIn 1959, a biological model proposed by Nobel laureates Hubel and Wiesel was based on their discovery of two types of cells in the primary visual cortex: simple cells and complex cells. [10]\r\n\r\nThe first functional networks with many layers were published by Ivakhnenko and Lapa in 1965, becoming the Group Method of Data Handling. [11][12][13]\r\n\r\nNeural network research stagnated after machine learning research by Minsky and Papert (1969),[14] who discovered two key issues with the computational machines that processed neural networks. The first was that basic perceptrons were incapable of processing the exclusive-or circuit. The second was that computers didn't have enough processing power to effectively handle the work required by large neural networks. Neural network research slowed until computers achieved far greater processing power. "}