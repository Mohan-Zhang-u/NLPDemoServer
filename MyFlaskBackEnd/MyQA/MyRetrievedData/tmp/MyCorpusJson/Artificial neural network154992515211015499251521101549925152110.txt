{"id": "Artificial neural network154992515211015499251521101549925152110.txt", "text": "The softmax function is defined as {\\displaystyle p_{j}={\\frac {\\exp(x_{j})}{\\sum _{k}\\exp(x_{k})}}} p_{j}={\\frac {\\exp(x_{j})}{\\sum _{k}\\exp(x_{k})}} where {\\displaystyle p_{j}} p_{j} represents the class probability (output of the unit {\\displaystyle j} j) and {\\displaystyle x_{j}} x_{j} and {\\displaystyle x_{k}} x_{k} represent the total input to units {\\displaystyle j} j and {\\displaystyle k} k of the same level respectively. Cross entropy is defined as {\\displaystyle C=-\\sum _{j}d_{j}\\log(p_{j})} C=-\\sum _{j}d_{j}\\log(p_{j}) where {\\displaystyle d_{j}} d_{j} represents the target probability for output unit {\\displaystyle j} j and {\\displaystyle p_{j}} p_{j} is the probability output for {\\displaystyle j} j after applying the activation function. [73]\r\n\r\nThese can be used to output object bounding boxes in the form of a binary mask. They are also used for multi-scale regression to increase localization precision. DNN-based regression can learn features that capture geometric information in addition to serving as a good classifier. They remove the requirement to explicitly model parts and their relations. This helps to broaden the variety of objects that can be learned. The model consists of multiple layers, each of which has a rectified linear unit as its activation function for non-linear transformation. Some layers are convolutional, while others are fully connected. Every convolutional layer has an additional max pooling. The network is trained to minimize L2 error for predicting the mask ranging over the entire training set containing bounding boxes represented as masks. Alternatives to backpropagation include Extreme Learning Machines,[74] \"No-prop\" networks,[75] training without backtracking,[76] \"weightless\" networks,[77][78] and non-connectionist neural networks. Learning paradigms\r\nThe three major learning paradigms each correspond to a particular learning task. These are supervised learning, unsupervised learning and reinforcement learning. Supervised learning\r\nSupervised learning uses a set of example pairs {\\displaystyle (x,y),x\\in X,y\\in Y}  (x, y), x \\in X, y \\in Y and the aim is to find a function {\\displaystyle f:X\\rightarrow Y}  f : X \\rightarrow Y  in the allowed class of functions that matches the examples. In other words, we wish to infer the mapping implied by the data; the cost function is related to the mismatch between our mapping and the data and it implicitly contains prior knowledge about the problem domain. [79]\r\n\r\nA commonly used cost is the mean-squared error, which tries to minimize the average squared error between the network's output, {\\displaystyle f(x)} f(x), and the target value {\\displaystyle y} y over all the example pairs. "}