{"id": "Artificial neural network154992515210915499251521091549925152109154992515210915499251521091549925152109.txt", "text": "A widely used type of composition is the nonlinear weighted sum, where {\\displaystyle \\textstyle f(x)=K\\left(\\sum _{i}w_{i}g_{i}(x)\\right)} \\textstyle f(x)=K\\left(\\sum _{i}w_{i}g_{i}(x)\\right), where {\\displaystyle \\textstyle K} \\textstyle K (commonly referred to as the activation function[55]) is some predefined function, such as the hyperbolic tangent or sigmoid function or softmax function or rectifier function. The important characteristic of the activation function is that it provides a smooth transition as input values change, i.e. a small change in input produces a small change in output. The following refers to a collection of functions {\\displaystyle \\textstyle g_{i}} \\textstyle g_{i} as a vector {\\displaystyle \\textstyle g=(g_{1},g_{2},\\ldots ,g_{n})} \\textstyle g=(g_{1},g_{2},\\ldots ,g_{n}). ANN dependency graph\r\nThis figure depicts such a decomposition of {\\displaystyle \\textstyle f} \\textstyle f, with dependencies between variables indicated by arrows. These can be interpreted in two ways. The first view is the functional view: the input {\\displaystyle \\textstyle x} \\textstyle x is transformed into a 3-dimensional vector {\\displaystyle \\textstyle h} \\textstyle h, which is then transformed into a 2-dimensional vector {\\displaystyle \\textstyle g} \\textstyle g, which is finally transformed into {\\displaystyle \\textstyle f} \\textstyle f. This view is most commonly encountered in the context of optimization. The second view is the probabilistic view: the random variable {\\displaystyle \\textstyle F=f(G)} \\textstyle F=f(G) depends upon the random variable {\\displaystyle \\textstyle G=g(H)} \\textstyle G=g(H), which depends upon {\\displaystyle \\textstyle H=h(X)} \\textstyle H=h(X), which depends upon the random variable {\\displaystyle \\textstyle X} \\textstyle X. This view is most commonly encountered in the context of graphical models. The two views are largely equivalent. In either case, for this particular architecture, the components of individual layers are independent of each other (e.g., the components of {\\displaystyle \\textstyle g} \\textstyle g are independent of each other given their input {\\displaystyle \\textstyle h} \\textstyle h). This naturally enables a degree of parallelism in the implementation. Two separate depictions of the recurrent ANN dependency graph\r\nNetworks such as the previous one are commonly called feedforward, because their graph is a directed acyclic graph. Networks with cycles are commonly called recurrent. Such networks are commonly depicted in the manner shown at the top of the figure, where {\\displaystyle \\textstyle f} \\textstyle f is shown as being dependent upon itself. However, an implied temporal dependence is not shown. Learning\r\nSee also: Mathematical optimization, Estimation theory, and Machine learning\r\nThe possibility of learning has attracted the most interest in neural networks. "}