{"id": "Deep learning15499251520371549925152037.txt", "text": "At first, the DNN creates a map of virtual neurons and assigns random numerical values, or \"weights\", to connections between them. The weights and inputs are multiplied and return an output between 0 and 1. If the network didn\u2019t accurately recognize a particular pattern, an algorithm would adjust the weights. [102] That way the algorithm can make certain parameters more influential, until it determines the correct mathematical manipulation to fully process the data. Recurrent neural networks (RNNs), in which data can flow in any direction, are used for applications such as language modeling. [103][104][105][106][107] Long short-term memory is particularly effective for this use. [50][108]\r\n\r\nConvolutional deep neural networks (CNNs) are used in computer vision. [109] CNNs also have been applied to acoustic modeling for automatic speech recognition (ASR). [66]\r\n\r\nChallenges\r\nAs with ANNs, many issues can arise with naively trained DNNs. Two common issues are overfitting and computation time. DNNs are prone to overfitting because of the added layers of abstraction, which allow them to model rare dependencies in the training data. Regularization methods such as Ivakhnenko's unit pruning[28] or weight decay ( {\\displaystyle \\ell _{2}} \\ell _{2}-regularization) or sparsity ( {\\displaystyle \\ell _{1}} \\ell _{1}-regularization) can be applied during training to combat overfitting. [110] Alternatively dropout regularization randomly omits units from the hidden layers during training. This helps to exclude rare dependencies. [111] Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting. [112]\r\n\r\nDNNs must consider many training parameters, such as the size (number of layers and number of units per layer), the learning rate, and initial weights. Sweeping through the parameter space for optimal parameters may not be feasible due to the cost in time and computational resources. Various tricks, such as batching (computing the gradient on several training examples at once rather than individual examples)[113] speed up computation. Large processing capabilities of many-core architectures (such as, GPUs or the Intel Xeon Phi) have produced significant speedups in training, because of the suitability of such processing architectures for the matrix and vector computations. [114][115]\r\n\r\nAlternatively, engineers may look for other types of neural networks with more straightforward and convergent training algorithms. CMAC (cerebellar model articulation controller) is one such kind of neural network. It doesn't require learning rates or randomized initial weights for CMAC. The training process can be guaranteed to converge in one step with a new batch of data, and the computational complexity of the training algorithm is linear with respect to the number of neurons involved. [116][117]\r\n\r\nApplications\r\n\r\nIt has been suggested that this section be split out into another article titled Applications of Deep Learning. (Discuss) (June 2018)\r\nAutomatic speech recognition\r\nMain article: Speech recognition\r\nLarge-scale automatic speech recognition is the first and most convincing successful case of deep learning. "}