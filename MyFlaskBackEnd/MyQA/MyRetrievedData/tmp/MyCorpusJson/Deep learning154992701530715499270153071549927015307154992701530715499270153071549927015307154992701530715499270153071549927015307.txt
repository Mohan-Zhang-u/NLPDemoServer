{"id": "Deep learning154992701530715499270153071549927015307154992701530715499270153071549927015307154992701530715499270153071549927015307.txt", "text": "DNNs have proven themselves capable, for example, of a) identifying the style period of a given painting, b) \"capturing\" the style of a given painting and applying it in a visually pleasing manner to an arbitrary photograph, and c) generating striking imagery based on random visual input fields. [129][130]\r\n\r\nNatural language processing\r\nMain article: Natural language processing\r\nNeural networks have been used for implementing language models since the early 2000s. [103][131] LSTM helped to improve machine translation and language modeling. [104][105][106]\r\n\r\nOther key techniques in this field are negative sampling[132] and word embedding. Word embedding, such as word2vec, can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a vector space. Using word embedding as an RNN input layer allows the network to parse sentences and phrases using an effective compositional vector grammar. A compositional vector grammar can be thought of as probabilistic context free grammar (PCFG) implemented by an RNN. [133] Recursive auto-encoders built atop word embeddings can assess sentence similarity and detect paraphrasing. [133] Deep neural architectures provide the best results for constituency parsing,[134] sentiment analysis,[135] information retrieval,[136][137] spoken language understanding,[138] machine translation,[104][139] contextual entity linking,[139] writing style recognition,[140] Text classification and others. [141]\r\n\r\nRecent developments generalize word embedding to sentence embedding. Google Translate (GT) uses a large end-to-end long short-term memory network. [142][143][144][145][146][147] Google Neural Machine Translation (GNMT) uses an example-based machine translation method in which the system \"learns from millions of examples. \"[143] It translates \"whole sentences at a time, rather than pieces. Google Translate supports over one hundred languages. [143] The network encodes the \"semantics of the sentence rather than simply memorizing phrase-to-phrase translations\". [143][148] GT uses English as an intermediate between most language pairs. [148]\r\n\r\nDrug discovery and toxicology\r\nFor more information, see Drug discovery and Toxicology. A large percentage of candidate drugs fail to win regulatory approval. These failures are caused by insufficient efficacy (on-target effect), undesired interactions (off-target effects), or unanticipated toxic effects. [149][150] Research has explored use of deep learning to predict the biomolecular targets,[85][86] off-targets, and toxic effects of environmental chemicals in nutrients, household products and drugs. [87][88][89]\r\n\r\nAtomNet is a deep learning system for structure-based rational drug design. [151] AtomNet was used to predict novel candidate biomolecules for disease targets such as the Ebola virus[152] and multiple sclerosis. "}