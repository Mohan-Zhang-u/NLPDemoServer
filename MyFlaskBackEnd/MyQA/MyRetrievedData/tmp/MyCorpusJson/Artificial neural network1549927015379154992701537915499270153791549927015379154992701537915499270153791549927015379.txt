{"id": "Artificial neural network1549927015379154992701537915499270153791549927015379154992701537915499270153791549927015379.txt", "text": "Given a specific task to solve, and a class of functions {\\displaystyle \\textstyle F} \\textstyle F, learning means using a set of observations to find {\\displaystyle \\textstyle f^{*}\\in F} \\textstyle f^{*}\\in F which solves the task in some optimal sense. This entails defining a cost function {\\displaystyle \\textstyle C:F\\rightarrow \\mathbb {R} } \\textstyle C:F\\rightarrow \\mathbb {R}  such that, for the optimal solution {\\displaystyle \\textstyle f^{*}} \\textstyle f^{*}, {\\displaystyle \\textstyle C(f^{*})\\leq C(f)} \\textstyle C(f^{*})\\leq C(f) {\\displaystyle \\textstyle \\forall f\\in F} \\textstyle \\forall f\\in F \u2013 i.e., no solution has a cost less than the cost of the optimal solution (see mathematical optimization). The cost function {\\displaystyle \\textstyle C} \\textstyle C is an important concept in learning, as it is a measure of how far away a particular solution is from an optimal solution to the problem to be solved. Learning algorithms search through the solution space to find a function that has the smallest possible cost. For applications where the solution is data dependent, the cost must necessarily be a function of the observations, otherwise the model would not relate to the data. It is frequently defined as a statistic to which only approximations can be made. As a simple example, consider the problem of finding the model {\\displaystyle \\textstyle f} \\textstyle f, which minimizes {\\displaystyle \\textstyle C=E\\left[(f(x)-y)^{2}\\right]} \\textstyle C=E\\left[(f(x)-y)^{2}\\right], for data pairs {\\displaystyle \\textstyle (x,y)} \\textstyle (x,y) drawn from some distribution {\\displaystyle \\textstyle {\\mathcal {D}}} \\textstyle {\\mathcal {D}}. In practical situations we would only have {\\displaystyle \\textstyle N} \\textstyle N samples from {\\displaystyle \\textstyle {\\mathcal {D}}} \\textstyle {\\mathcal {D}} and thus, for the above example, we would only minimize {\\displaystyle \\textstyle {\\hat {C}}={\\frac {1}{N}}\\sum _{i=1}^{N}(f(x_{i})-y_{i})^{2}} \\textstyle {\\hat {C}}={\\frac {1}{N}}\\sum _{i=1}^{N}(f(x_{i})-y_{i})^{2}. Thus, the cost is minimized over a sample of the data rather than the entire distribution. When {\\displaystyle \\textstyle N\\rightarrow \\infty } \\textstyle N\\rightarrow \\infty  some form of online machine learning must be used, where the cost is reduced as each new example is seen. While online machine learning is often used when {\\displaystyle \\textstyle {\\mathcal {D}}} \\textstyle {\\mathcal {D}} is fixed, it is most useful in the case where the distribution changes slowly over time. In neural network methods, some form of online machine learning is frequently used for finite datasets. "}