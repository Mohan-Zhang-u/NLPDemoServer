{"id": "Artificial neural network15499251521101549925152110154992515211015499251521101549925152110.txt", "text": "More formally the environment is modeled as a Markov decision process (MDP) with states {\\displaystyle \\textstyle {s_{1},...,s_{n}}\\in S} \\textstyle {s_{1},...,s_{n}}\\in S and actions {\\displaystyle \\textstyle {a_{1},...,a_{m}}\\in A} \\textstyle {a_{1},...,a_{m}}\\in A with the following probability distributions: the instantaneous cost distribution {\\displaystyle \\textstyle P(c_{t}|s_{t})} \\textstyle P(c_{t}|s_{t}), the observation distribution {\\displaystyle \\textstyle P(x_{t}|s_{t})} \\textstyle P(x_{t}|s_{t}) and the transition {\\displaystyle \\textstyle P(s_{t+1}|s_{t},a_{t})} \\textstyle P(s_{t+1}|s_{t},a_{t}), while a policy is defined as the conditional distribution over actions given the observations. Taken together, the two then define a Markov chain (MC). The aim is to discover the policy (i.e., the MC) that minimizes the cost. Artificial neural networks are frequently used in reinforcement learning as part of the overall algorithm. [80][81] Dynamic programming was coupled with Artificial neural networks (giving neurodynamic programming) by Bertsekas and Tsitsiklis[82] and applied to multi-dimensional nonlinear problems such as those involved in vehicle routing,[83] natural resources management[84][85] or medicine[86] because of the ability of Artificial neural networks to mitigate losses of accuracy even when reducing the discretization grid density for numerically approximating the solution of the original control problems. Tasks that fall within the paradigm of reinforcement learning are control problems, games and other sequential decision making tasks. Learning algorithms\r\nSee also: Machine learning\r\nTraining a neural network model essentially means selecting one model from the set of allowed models (or, in a Bayesian framework, determining a distribution over the set of allowed models) that minimizes the cost. Numerous algorithms are available for training neural network models; most of them can be viewed as a straightforward application of optimization theory and statistical estimation. Most employ some form of gradient descent, using backpropagation to compute the actual gradients. This is done by simply taking the derivative of the cost function with respect to the network parameters and then changing those parameters in a gradient-related direction. Backpropagation training algorithms fall into three categories:\r\n\r\nsteepest descent (with variable learning rate and momentum, resilient backpropagation);\r\nquasi-Newton (Broyden-Fletcher-Goldfarb-Shanno, one step secant);\r\nLevenberg-Marquardt and conjugate gradient (Fletcher-Reeves update, Polak-Ribi\u00e9re update, Powell-Beale restart, scaled conjugate gradient). [87]\r\nEvolutionary methods,[88] gene expression programming,[89] simulated annealing,[90] expectation-maximization, non-parametric methods and particle swarm optimization[91] are other methods for training neural networks. Convergent recursive learning algorithm\r\nThis is a learning method specially designed for cerebellar model articulation controller (CMAC) neural networks. "}