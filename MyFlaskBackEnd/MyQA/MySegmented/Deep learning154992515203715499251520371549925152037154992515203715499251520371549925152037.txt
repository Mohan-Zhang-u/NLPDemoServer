A 1995 description stated, "...the infant's brain seems to organize itself under the influence of waves of so-called trophic-factors ... different regions of the brain become connected sequentially, with one layer of tissue maturing before another and so on until the whole brain is mature. "[171]

A variety of approaches have been used to investigate the plausibility of deep learning models from a neurobiological perspective. On the one hand, several variants of the backpropagation algorithm have been proposed in order to increase its processing realism. [172][173] Other researchers have argued that unsupervised forms of deep learning, such as those based on hierarchical generative models and deep belief networks, may be closer to biological reality. [174][175] In this respect, generative neural network models have been related to neurobiological evidence about sampling-based processing in the cerebral cortex. [176]

Although a systematic comparison between the human brain organization and the neuronal encoding in deep networks has not yet been established, several analogies have been reported. For example, the computations performed by deep learning units could be similar to those of actual neurons[177][178] and neural populations. [179] Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system[180] both at the single-unit[181] and at the population[182] levels. Commercial activity
Many organizations employ deep learning for particular applications. Facebook's AI lab performs tasks such as automatically tagging uploaded pictures with the names of the people in them. [183]

Google's DeepMind Technologies developed a system capable of learning how to play Atari video games using only pixels as data input. In 2015 they demonstrated their AlphaGo system, which learned the game of Go well enough to beat a professional Go player. [184][185][186] Google Translate uses an LSTM to translate between more than 100 languages. In 2015, Blippar demonstrated a mobile augmented reality application that uses deep learning to recognize objects in real time. [187]

As of 2008,[188] researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement, or TAMER, which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor. [166]

First developed as TAMER, a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers. Deep TAMER used deep learning to provide a robot the ability to learn new tasks through observation. [166]

Using Deep TAMER, a robot learned a task with a human trainer, watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer, who provided feedback such as “good job” and “bad job.”[189]

Criticism and comment
Deep learning has attracted both criticism and comment, in some cases from outside the field of computer science. Theory
See also: Explainable AI
A main criticism concerns the lack of theory surrounding some methods. 