Each connection is assigned a weight {\displaystyle w_{ij}} w_{ij}. [52] Sometimes a bias term added to total weighted sum of inputs to serve as threshold to shift the activation function. [53]

Propagation function
The propagation function computes the input {\displaystyle p_{j}(t)} {\displaystyle p_{j}(t)} to the neuron {\displaystyle j} j from the outputs {\displaystyle o_{i}(t)} {\displaystyle o_{i}(t)} of predecessor neurons and typically has the form[52]

{\displaystyle p_{j}(t)=\sum _{i}o_{i}(t)w_{ij}} {\displaystyle p_{j}(t)=\sum _{i}o_{i}(t)w_{ij}}. When a bias value added with the function, the above form changes to following [54]

{\displaystyle p_{j}(t)=\sum _{i}o_{i}(t)w_{ij}+w_{0j}} {\displaystyle p_{j}(t)=\sum _{i}o_{i}(t)w_{ij}+w_{0j}} , where {\displaystyle w_{0j}} {\displaystyle w_{0j}} is a bias. Learning rule
The learning rule is a rule or an algorithm which modifies the parameters of the neural network, in order for a given input to the network to produce a favored output. This learning process typically amounts to modifying the weights and thresholds of the variables within the network. [52]

Neural networks as functions
See also: Graphical models
Neural network models can be viewed as simple mathematical models defining a function {\displaystyle \textstyle f:X\rightarrow Y} {\displaystyle \textstyle f:X\rightarrow Y} or a distribution over {\displaystyle \textstyle X} \textstyle X or both {\displaystyle \textstyle X} \textstyle X and {\displaystyle \textstyle Y} \textstyle Y. Sometimes models are intimately associated with a particular learning rule. A common use of the phrase "ANN model" is really the definition of a class of such functions (where members of the class are obtained by varying parameters, connection weights, or specifics of the architecture such as the number of neurons or their connectivity). Mathematically, a neuron's network function {\displaystyle \textstyle f(x)} \textstyle f(x) is defined as a composition of other functions {\displaystyle \textstyle g_{i}(x)} \textstyle g_{i}(x), that can further be decomposed into other functions. This can be conveniently represented as a network structure, with arrows depicting the dependencies between functions. 